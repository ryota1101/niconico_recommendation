{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前学習済みモデル\n",
    "PRETRAINED_MODEL_NAME = \"sonoisa/byt5-small-japanese\"\n",
    "\n",
    "\n",
    "# 転移学習済みモデルを保存する場所\n",
    "MODEL_DIR = \"byt5_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# 乱数シードの設定\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU利用有無\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "# 各種ハイパーパラメータ\n",
    "args_dict = dict(\n",
    "    data_dir=\"data/\",  # データセットのディレクトリ\n",
    "    model_name_or_path=PRETRAINED_MODEL_NAME,\n",
    "    tokenizer_name_or_path=PRETRAINED_MODEL_NAME,\n",
    "\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    # max_input_length=512,\n",
    "    # max_target_length=4,\n",
    "    # train_batch_size=8,\n",
    "    # eval_batch_size=8,\n",
    "    # num_train_epochs=4,\n",
    "\n",
    "    n_gpu=1 if USE_GPU else 0,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False,\n",
    "    opt_level='O1',\n",
    "    max_grad_norm=1.0,\n",
    "    seed=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#niconicoデータ用に改変\n",
    "class TsvDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, type_path, input_max_len=512, target_max_len=512):\n",
    "        self.file_path = os.path.join(data_dir, type_path)\n",
    "        \n",
    "        self.input_max_len = input_max_len\n",
    "        self.target_max_len = target_max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._build()\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        source_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": source_mask, \n",
    "                \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "    def _make_record(self, body, genre_id):\n",
    "        # コメント分類タスク用の入出力形式に変換する。\n",
    "        input = f\"{body}\"\n",
    "        target = f\"{genre_id}\"\n",
    "        return input, target\n",
    "  \n",
    "    def _build(self):\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                assert len(line) == 2\n",
    "                assert len(line[0]) > 0\n",
    "                assert len(line[1]) > 0\n",
    "\n",
    "                body = line[0]\n",
    "                genre_id = line[1]\n",
    "\n",
    "                input, target = self._make_record(body, genre_id)\n",
    "\n",
    "                tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                    [input], max_length=self.input_max_len, truncation=True, \n",
    "                    padding=\"max_length\", return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                    [target], max_length=self.target_max_len, truncation=True, \n",
    "                    padding=\"max_length\", return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                self.inputs.append(tokenized_inputs)\n",
    "                self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, MT5ForConditionGeneration\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"sonoisa/byt5-small-japanese\")\n",
    "\n",
    "# model = MT5ForConditionGeneration.from_pretrained(\"sonoisa/byt5-small-japanese\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sonoisa/byt5-small-japanese\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sonoisa/byt5-small-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. 入力データの元になる文字列\n",
      "初見で何故か怖かった…</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "B. 入力データ（Aの文字列がトークナイズされたトークンID列）\n",
      "tensor([232, 139, 160, 235, 169, 142, 230, 132, 170, 231, 192, 152, 233, 152,\n",
      "        136, 230, 132, 142, 233, 131, 153, 230, 132, 142, 230, 132, 166, 230,\n",
      "        132, 162, 229, 131, 169,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n",
      "\n",
      "C. 出力データの元になる文字列\n",
      "3</s><pad><pad>\n",
      "\n",
      "D. 出力データ（Cの文字列がトークナイズされたトークンID列）\n",
      "tensor([54,  1,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# テストデータセットの読み込み\n",
    "train_dataset = TsvDataset(tokenizer, args_dict[\"data_dir\"], \"nico_min_train.tsv\", \n",
    "                           input_max_len=100, target_max_len=4)\n",
    "for data in train_dataset:\n",
    "    print(\"A. 入力データの元になる文字列\")\n",
    "    print(tokenizer.decode(data[\"source_ids\"]))\n",
    "    print()\n",
    "    print(\"B. 入力データ（Aの文字列がトークナイズされたトークンID列）\")\n",
    "    print(data[\"source_ids\"])\n",
    "    print()\n",
    "    print(\"C. 出力データの元になる文字列\")\n",
    "    print(tokenizer.decode(data[\"target_ids\"]))\n",
    "    print()\n",
    "    print(\"D. 出力データ（Cの文字列がトークナイズされたトークンID列）\")\n",
    "    print(data[\"target_ids\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用するファイル名設定\n",
    "train_file = \"nico_20000_train.tsv\"\n",
    "test_file = \"nico_20000_test.tsv\"\n",
    "dev_file = \"nico_20000_dev.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByT5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # 事前学習済みモデルの読み込み\n",
    "        #self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(hparams.model_name_or_path)\n",
    "\n",
    "\n",
    "        # トークナイザーの読み込み\n",
    "        #self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path, is_fast=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.tokenizer_name_or_path, is_fast=True)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, \n",
    "                decoder_attention_mask=None, labels=None):\n",
    "        \"\"\"順伝搬\"\"\"\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        \"\"\"ロス計算\"\"\"\n",
    "        labels = batch[\"target_ids\"]\n",
    "\n",
    "        # All labels set to -100 are ignored (masked), \n",
    "        # the loss is only computed for labels in [0, ..., config.vocab_size]\n",
    "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"訓練ステップ処理\"\"\"\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"バリデーションステップ処理\"\"\"\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    # def validation_epoch_end(self, outputs):\n",
    "    #     \"\"\"バリデーション完了処理\"\"\"\n",
    "    #     avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    #     self.log(\"val_loss\", avg_loss, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"テストステップ処理\"\"\"\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return {\"test_loss\": loss}\n",
    "\n",
    "    # def test_epoch_end(self, outputs):\n",
    "    #     \"\"\"テスト完了処理\"\"\"\n",
    "    #     avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "    #     self.log(\"test_loss\", avg_loss, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"オプティマイザーとスケジューラーを作成する\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                            if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                            if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                          lr=self.hparams.learning_rate, \n",
    "                          eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.hparams.warmup_steps, \n",
    "            num_training_steps=self.t_total\n",
    "        )\n",
    "\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n",
    "\n",
    "    def get_dataset(self, tokenizer, type_path, args):\n",
    "        \"\"\"データセットを作成する\"\"\"\n",
    "        return TsvDataset(\n",
    "            tokenizer=tokenizer, \n",
    "            data_dir=args.data_dir, \n",
    "            type_path=type_path, \n",
    "            input_max_len=args.max_input_length,\n",
    "            target_max_len=args.max_target_length)\n",
    "\n",
    "##ここのファイルパスのところをniconicoに変更\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"初期設定（データセットの読み込み）\"\"\"\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_dataset = self.get_dataset(tokenizer=self.tokenizer, \n",
    "                                             type_path=train_file, args=self.hparams)\n",
    "            self.train_dataset = train_dataset\n",
    "\n",
    "            val_dataset = self.get_dataset(tokenizer=self.tokenizer, \n",
    "                                           type_path=dev_file, args=self.hparams)\n",
    "            self.val_dataset = val_dataset\n",
    "\n",
    "            self.t_total = (\n",
    "                (len(train_dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "                // self.hparams.gradient_accumulation_steps\n",
    "                * float(self.hparams.num_train_epochs)\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"訓練データローダーを作成する\"\"\"\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.hparams.train_batch_size, \n",
    "                          drop_last=True, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"バリデーションデータローダーを作成する\"\"\"\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.hparams.eval_batch_size, \n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習に用いるハイパーパラメータを設定する\n",
    "args_dict.update({\n",
    "    \"max_input_length\":  100,  # 入力文の最大トークン数\n",
    "    \"max_target_length\": 4,  # 出力文の最大トークン数\n",
    "    \"train_batch_size\":  8,\n",
    "    \"eval_batch_size\":   8,\n",
    "    \"num_train_epochs\":  4,\n",
    "    })\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "# checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "#     \"/content/checkpoints\", \n",
    "#     monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    "# )\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    # checkpoint_callback=checkpoint_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/mnt/data1/home/ooga/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                        | Params\n",
      "------------------------------------------------------\n",
      "0 | model | MT5ForConditionalGeneration | 299 M \n",
      "------------------------------------------------------\n",
      "299 M     Trainable params\n",
      "0         Non-trainable params\n",
      "299 M     Total params\n",
      "1,198.551 Total estimated model params size (MB)\n",
      "\n",
      "  | Name  | Type                        | Params\n",
      "------------------------------------------------------\n",
      "0 | model | MT5ForConditionalGeneration | 299 M \n",
      "------------------------------------------------------\n",
      "299 M     Trainable params\n",
      "0         Non-trainable params\n",
      "299 M     Total params\n",
      "1,198.551 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db910b41c2004be7b8c34046e362b8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 転移学習の実行（GPUを利用すれば1エポック10分程度）\n",
    "model = ByT5FineTuner(args)\n",
    "trainer = pl.Trainer(**train_params)\n",
    "trainer.fit(model)\n",
    "\n",
    "# 最終エポックのモデルを保存\n",
    "model.tokenizer.save_pretrained(MODEL_DIR)\n",
    "model.model.save_pretrained(MODEL_DIR)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# トークナイザー（SentencePiece）\n",
    "#tokenizer = T5Tokenizer.from_pretrained(MODEL_DIR, is_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, is_fast=True)\n",
    "\n",
    "# 学習済みモデル\n",
    "#trained_model = T5ForConditionalGeneration.from_pretrained(MODEL_DIR)\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)#\"google/byt5-small\")\n",
    "\n",
    "# GPUの利用有無\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "if USE_GPU:\n",
    "    trained_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7187aa1cff184aefb7ca5fefeb46737c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "# テストデータの読み込み\n",
    "test_dataset = TsvDataset(tokenizer, args_dict[\"data_dir\"], test_file, \n",
    "                          input_max_len=args.max_input_length, \n",
    "                          target_max_len=args.max_target_length)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4)\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "outputs = []\n",
    "confidences = []\n",
    "targets = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids = batch['source_ids']\n",
    "    input_mask = batch['source_mask']\n",
    "    if USE_GPU:\n",
    "        input_ids = input_ids.cuda()\n",
    "        input_mask = input_mask.cuda()\n",
    "\n",
    "    outs = trained_model.generate(input_ids=input_ids, \n",
    "        attention_mask=input_mask, \n",
    "        max_length=args.max_target_length,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True, \n",
    "                            clean_up_tokenization_spaces=False) \n",
    "                for ids in outs.sequences]\n",
    "    #conf = [s.cpu().item() for s in torch.exp(outs.sequences_scores)]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True, \n",
    "                               clean_up_tokenization_spaces=False) \n",
    "                for ids in batch[\"target_ids\"]]\n",
    "\n",
    "    outputs.extend(dec)\n",
    "    #confidences.extend(conf)\n",
    "    targets.extend(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       735\n",
      "           1       0.95      0.95      0.95       728\n",
      "           2       0.93      0.92      0.92       792\n",
      "           3       0.94      0.93      0.93       745\n",
      "\n",
      "    accuracy                           0.93      3000\n",
      "   macro avg       0.93      0.93      0.93      3000\n",
      "weighted avg       0.93      0.93      0.93      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(targets, outputs))#nico_20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      4739\n",
      "           1       0.97      0.97      0.97      6338\n",
      "           2       0.92      0.92      0.92      3134\n",
      "           3       0.95      0.94      0.94      2754\n",
      "\n",
      "    accuracy                           0.95     16965\n",
      "   macro avg       0.94      0.94      0.94     16965\n",
      "weighted avg       0.95      0.95      0.95     16965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(targets, outputs))#nico_main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
