{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import  fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def unicode_normalize(cls, s):\n",
    "    pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "    def norm(c):\n",
    "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "\n",
    "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "    s = re.sub('－', '-', s)\n",
    "    return s\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    s = re.sub('[ 　]+', ' ', s)\n",
    "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                      '\\u3040-\\u309F',  # HIRAGANA\n",
    "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                      ))\n",
    "    basic_latin = '\\u0000-\\u007F'\n",
    "\n",
    "    def remove_space_between(cls1, cls2, s):\n",
    "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "        while p.search(s):\n",
    "            s = p.sub(r'\\1\\2', s)\n",
    "        return s\n",
    "\n",
    "    s = remove_space_between(blocks, blocks, s)\n",
    "    s = remove_space_between(blocks, basic_latin, s)\n",
    "    s = remove_space_between(basic_latin, blocks, s)\n",
    "    return s\n",
    "\n",
    "def normalize_neologd(s):\n",
    "    s = s.strip()\n",
    "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
    "\n",
    "    def maketrans(f, t):\n",
    "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "\n",
    "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
    "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
    "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
    "    s = s.translate(\n",
    "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
    "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
    "\n",
    "    s = remove_extra_spaces(s)\n",
    "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
    "    s = re.sub('[’]', '\\'', s)\n",
    "    s = re.sub('[”]', '\"', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "\n",
    "target_genres = [\"dokujo-tsushin\",\n",
    "                 \"it-life-hack\",\n",
    "                 \"kaden-channel\",\n",
    "                 \"livedoor-homme\",\n",
    "                 \"movie-enter\",\n",
    "                 \"peachy\",\n",
    "                 \"smax\",\n",
    "                 \"sports-watch\",\n",
    "                 \"topic-news\"]\n",
    "\n",
    "def remove_brackets(text):\n",
    "    text = re.sub(r\"(^【[^】]*】)|(【[^】]*】$)\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    assert \"\\n\" not in text and \"\\r\" not in text\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.strip()\n",
    "    text = normalize_neologd(text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def read_title_body(file):\n",
    "    next(file)\n",
    "    next(file)\n",
    "    title = next(file).decode(\"utf-8\").strip()\n",
    "    title = normalize_text(remove_brackets(title))\n",
    "    body = normalize_text(\" \".join([line.decode(\"utf-8\").strip() for line in file.readlines()]))\n",
    "    return title, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "def wakati_pred(text):\n",
    "    #tagger = MeCab.Tagger('')\n",
    "    tagger = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd -u /mnt/data1/home/ooga/mydic/foo.dic')\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(text)\n",
    "    stopword = [\"これ\", \"ちょ\",\"さん\"]\n",
    "    through = [\"好き\", \"格好\",\"すき\",\"嫌\",\"かわい\",\"かわえー\",\"だいすきだー\",\"大好き\",\"イケメン\",\"IKEMEN\",\"大好きだー\",\"きもい\",\"きっも\"]\n",
    "    word_list = []\n",
    "    meishi = 0\n",
    "    other = 0\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "        stop = node.feature.split(\",\")[6]\n",
    "        #print(pos,stop)\n",
    "        #if pos in [\"動詞\", \"形容詞\",\"感動詞\"]:\n",
    "        if (pos in [\"名詞\",\"動詞\", \"形容詞\",\"感動詞\",\"助動詞\",\"助詞\",\"副詞\",\"フィラー\"]):\n",
    "        #[\"動詞\", \"形容詞\",\"感動詞\",\"名詞\", \"副詞\", \"助詞\", \"接続詞\", \"助動詞\", \"連体詞\", \"感動詞\"]:\n",
    "            word = node.surface\n",
    "            word_list.append(word)\n",
    "            if(pos == \"名詞\" and (stop not in through)):\n",
    "                meishi += 1\n",
    "            else:\n",
    "                other += 1\n",
    "        node = node.next\n",
    "    #print(meishi, other)\n",
    "    if meishi == 1 and other == 0:\n",
    "        word_list = []\n",
    "        #print(\"消す\")\n",
    "    return \" \".join(word_list)\n",
    "\n",
    "def concat_df(main_df, sub_df):\n",
    "    df_list = [main_df, sub_df]\n",
    "    df_concat = pd.concat(df_list, ignore_index=True)\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wakati_all(text):\n",
    "    #tagger = MeCab.Tagger('')\n",
    "    tagger = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd -u /mnt/data1/home/ooga/mydic/foo.dic')\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(text)\n",
    "    stopword = [\"これ\", \"ちょ\",\"さん\"]\n",
    "    word_list = []\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "        stop = node.feature.split(\",\")[6]\n",
    "        if pos in [\"動詞\", \"形容詞\",\"感動詞\",\"名詞\", \"副詞\", \"助詞\", \"接続詞\", \"助動詞\", \"連体詞\", \"感動詞\",\"フィラー\"]:\n",
    "            word = node.surface\n",
    "            word_list.append(word)\n",
    "        node = node.next\n",
    "    return \" \".join(word_list)\n",
    "    #return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここから\n",
    "df1 = pd.read_csv('ByT5/data/nico_20000_train.tsv', sep='\\t', header=None)\n",
    "df_1 = pd.read_csv('ByT5/data/nico_20000_dev.tsv', sep='\\t', header=None)\n",
    "df2 = pd.read_csv('ByT5/data/nico_20000_test.tsv', sep='\\t', header=None)\n",
    "df1 = concat_df(df1, df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns={0: \"content\", 1:\"emotion\"})\n",
    "df2 = df2.rename(columns={0: \"content\", 1:\"emotion\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"wakati\"] = df1[\"content\"].apply(wakati_all)\n",
    "df2[\"wakati\"] = df2[\"content\"].apply(wakati_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4272\n",
       "0    4265\n",
       "3    4255\n",
       "2    4208\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df1))\n",
    "df1[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    792\n",
       "3    745\n",
       "0    735\n",
       "1    728\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df2))\n",
    "df2[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_Train = df1.content.values #文章の抽出\n",
    "s_Train = df1.wakati.values #わかち書き文章の抽出\n",
    "l_Train = df1.emotion.values #ラベルの抽出\n",
    "#s_Test = df2.content.values #文章の抽出\n",
    "s_Test = df2.wakati.values #わかち書き後文章の抽出\n",
    "l_Test = df2.emotion.values #ラベルの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#コメントをbowでベクトル化し，学習するモデル\n",
    "NB_model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "NB_model.fit(s_Train, l_Train)\n",
    "#NB_model = make_pipeline(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ここから word2vec + svm\n",
    "import gensim\n",
    "from gensim.models import word2vec \n",
    "import logging\n",
    "import sys\n",
    "import gensim.downloader as gendl\n",
    "import pyemd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    datapath(\"/mnt/data1/home/ooga/models/jawiki.all_vectors.300d.txt\"),\n",
    "    binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ベクトル作成\n",
    "def avg_feature_vector(comment, model, num_features=300):\n",
    "    li = comment.split(\" \")\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\") # 特徴ベクトルの入れ物を初期化\n",
    "    feature_vec_ins = np.zeros(300, dtype=\"float32\") # 特徴ベクトルの入れ物を初期化\n",
    "    for word in li:\n",
    "        if word in model:#辞書にない場合は省く\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if len(li) > 0:\n",
    "        feature_vec = np.divide(feature_vec, len(li))\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_train = []\n",
    "for x in range(len(s_Train)):\n",
    "    vec = avg_feature_vector(s_Train[x], model)\n",
    "    comm_train.append(vec)\n",
    "x_train = np.array(comm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_train = []\n",
    "for y in range(len(s_Test)):\n",
    "    vec = avg_feature_vector(s_Test[y], model)\n",
    "    comm_train.append(vec)\n",
    "x_test = np.array(comm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.001, 0.0001]},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "score = 'f1'\n",
    "clf = GridSearchCV(\n",
    "    SVC(), # 識別器 非線形SVM\n",
    "    tuned_parameters, # 最適化したいパラメータセット \n",
    "    cv=5, # 交差検定の回数\n",
    "    scoring='%s_weighted' % score ) # モデルの評価関数の指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n",
       "                          'kernel': ['rbf']}],\n",
       "             scoring='f1_weighted')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, l_Train) #最適化実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000.0, gamma=0.001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(gamma=0.001, C=1000.,kernel=\"rbf\")\n",
    "clf.fit(x_train, l_Train)\n",
    "##ここまで word2vec + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_Pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.74       735\n",
      "           1       0.75      0.74      0.75       728\n",
      "           2       0.75      0.72      0.74       792\n",
      "           3       0.75      0.73      0.74       745\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.74      0.74      0.74      3000\n",
      "weighted avg       0.74      0.74      0.74      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 評価レポート word2vec + svm\n",
    "print(classification_report(l_Test, l_Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここからsvm + bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "count.fit(df1['wakati'].values)\n",
    "x_train = count.transform(df1['wakati'].values)\n",
    "x_test = count.transform(df2['wakati'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.001, 0.0001]},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "score = 'f1'\n",
    "clf = GridSearchCV(\n",
    "    SVC(), # 識別器\n",
    "    tuned_parameters, # 最適化したいパラメータセット \n",
    "    cv=5, # 交差検定の回数\n",
    "    scoring='%s_weighted' % score ) # モデルの評価関数の指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n",
       "                          'kernel': ['rbf']}],\n",
       "             scoring='f1_weighted')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, l_Train) #最適化実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel=\"linear\", C=1.)\n",
    "clf.fit(x_train, l_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_Pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.82      0.75       735\n",
      "           1       0.89      0.87      0.88       728\n",
      "           2       0.87      0.83      0.85       792\n",
      "           3       0.85      0.77      0.81       745\n",
      "\n",
      "    accuracy                           0.82      3000\n",
      "   macro avg       0.83      0.82      0.82      3000\n",
      "weighted avg       0.83      0.82      0.82      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 評価レポート #bow + svm\n",
    "print(classification_report(l_Test, l_Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多項ナイーブベイズ 予測\n",
    "l_Pred = NB_model.predict(s_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec + svm\n",
    "l_Pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74      2425\n",
      "           1       0.79      0.78      0.79      2460\n",
      "           2       0.76      0.75      0.75      2534\n",
      "           3       0.74      0.75      0.75      2563\n",
      "\n",
      "    accuracy                           0.76      9982\n",
      "   macro avg       0.76      0.76      0.76      9982\n",
      "weighted avg       0.76      0.76      0.76      9982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(l_Test, l_Pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
